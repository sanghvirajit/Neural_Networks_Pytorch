{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\Rajit_Sanghvi\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# letâ€™s initialize the MNIST train and test set.\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 6000\n",
    "test_batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=mnist_trainset,\n",
    "                 batch_size=train_batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=mnist_testset,\n",
    "                batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=576, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(32,64, kernel_size=5)\n",
    "        self.maxpool = nn.MaxPool2d(2, stride=2)\n",
    "        self.fc1 = nn.Linear(3*3*64, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.maxpool(self.conv2(x)))\n",
    "        x = F.relu(self.maxpool(self.conv3(x)))\n",
    "        x = x.view(-1,3*3*64 )\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1) \n",
    "        return x\n",
    "    \n",
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6000, 1, 28, 28])\n",
      "torch.Size([6000, 10])\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_loader)\n",
    "X_batch, y_batch = next(it)\n",
    "print(X_batch.shape)\n",
    "print(cnn.forward(X_batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader):\n",
    "    \n",
    "    # Adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters()) # lr=0.001, betas=(0.9,0.999))\n",
    "    \n",
    "    # cross entropy function\n",
    "    error = nn.CrossEntropyLoss()\n",
    "\n",
    "    EPOCHS = 15\n",
    "        \n",
    "    for epoch in range(EPOCHS):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            \n",
    "            # wrapping tensors in variables,  If x is a Variable then x.data is a Tensor giving its value, \n",
    "            # and x.grad is another Variable holding the gradient of x with respect to some scalar value\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "                                    \n",
    "            # we need to set the gradients to zero before starting to do backpropragation\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # output of the model\n",
    "            output = model.forward(var_X_batch)\n",
    "                        \n",
    "            # Calculating the loss \n",
    "            loss = error(output, var_y_batch)\n",
    "            \n",
    "            # Let's do backpropogation, it will calculate all the gradients and save to x.grad\n",
    "            loss.backward()\n",
    "            \n",
    "            # Performs a single optimization step,  \n",
    "            #parameter update based on the current gradient (stored in .grad attribute of a parameter) and the update rule\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Total correct predictions\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            \n",
    "        \n",
    "        print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "            epoch, \n",
    "            (batch_idx+1)*(len(X_batch)), \n",
    "            len(train_loader.dataset), \n",
    "            100.*(batch_idx+1) / len(train_loader), \n",
    "            loss.data, \n",
    "            float(correct*100) / float(train_batch_size*(batch_idx+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [60000/60000 (100%)]\tLoss: 0.862470\t Accuracy:51.583%\n",
      "Epoch : 1 [60000/60000 (100%)]\tLoss: 0.415070\t Accuracy:80.622%\n",
      "Epoch : 2 [60000/60000 (100%)]\tLoss: 0.275241\t Accuracy:88.050%\n",
      "Epoch : 3 [60000/60000 (100%)]\tLoss: 0.191218\t Accuracy:91.747%\n",
      "Epoch : 4 [60000/60000 (100%)]\tLoss: 0.143435\t Accuracy:93.717%\n",
      "Epoch : 5 [60000/60000 (100%)]\tLoss: 0.113378\t Accuracy:95.088%\n",
      "Epoch : 6 [60000/60000 (100%)]\tLoss: 0.093364\t Accuracy:96.053%\n",
      "Epoch : 7 [60000/60000 (100%)]\tLoss: 0.078530\t Accuracy:96.710%\n",
      "Epoch : 8 [60000/60000 (100%)]\tLoss: 0.069828\t Accuracy:97.120%\n",
      "Epoch : 9 [60000/60000 (100%)]\tLoss: 0.063379\t Accuracy:97.438%\n",
      "Epoch : 10 [60000/60000 (100%)]\tLoss: 0.058118\t Accuracy:97.703%\n",
      "Epoch : 11 [60000/60000 (100%)]\tLoss: 0.053652\t Accuracy:97.938%\n",
      "Epoch : 12 [60000/60000 (100%)]\tLoss: 0.049730\t Accuracy:98.093%\n",
      "Epoch : 13 [60000/60000 (100%)]\tLoss: 0.046624\t Accuracy:98.215%\n",
      "Epoch : 14 [60000/60000 (100%)]\tLoss: 0.043438\t Accuracy:98.347%\n"
     ]
    }
   ],
   "source": [
    "fit(cnn,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:98.550 % \n"
     ]
    }
   ],
   "source": [
    "def evaluate(model):\n",
    "#model = cnn\n",
    "    correct = 0 \n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        \n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "               \n",
    "        output = model(test_imgs)\n",
    "        \n",
    "        predicted = torch.max(output,1)[1]\n",
    "        \n",
    "        correct += (predicted == test_labels).sum()\n",
    "        \n",
    "    print(\"Test accuracy:{:.3f} % \".format( float(correct*100) / (len(test_loader)*test_batch_size)))\n",
    "    \n",
    "evaluate(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
